{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Collecting Samples Complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path = './faces/user/' + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser as wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module cv2.face in cv2:\n",
      "\n",
      "NAME\n",
      "    cv2.face\n",
      "\n",
      "FUNCTIONS\n",
      "    BIF_create(...)\n",
      "        BIF_create([, num_bands[, num_rotations]]) -> retval\n",
      "        .   * @param num_bands The number of filter bands (<=8) used for computing BIF.\n",
      "        .   * @param num_rotations The number of image rotations for computing BIF.\n",
      "        .   * @returns Object for computing BIF.\n",
      "    \n",
      "    EigenFaceRecognizer_create(...)\n",
      "        EigenFaceRecognizer_create([, num_components[, threshold]]) -> retval\n",
      "        .   @param num_components The number of components (read: Eigenfaces) kept for this Principal\n",
      "        .   Component Analysis. As a hint: There's no rule how many components (read: Eigenfaces) should be\n",
      "        .   kept for good reconstruction capabilities. It is based on your input data, so experiment with the\n",
      "        .   number. Keeping 80 components should almost always be sufficient.\n",
      "        .   @param threshold The threshold applied in the prediction.\n",
      "        .   \n",
      "        .   ### Notes:\n",
      "        .   \n",
      "        .   -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n",
      "        .   color spaces.\n",
      "        .   -   **THE EIGENFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n",
      "        .   SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n",
      "        .   input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n",
      "        .   the images.\n",
      "        .   -   This model does not support updating.\n",
      "        .   \n",
      "        .   ### Model internal data:\n",
      "        .   \n",
      "        .   -   num_components see EigenFaceRecognizer::create.\n",
      "        .   -   threshold see EigenFaceRecognizer::create.\n",
      "        .   -   eigenvalues The eigenvalues for this Principal Component Analysis (ordered descending).\n",
      "        .   -   eigenvectors The eigenvectors for this Principal Component Analysis (ordered by their\n",
      "        .   eigenvalue).\n",
      "        .   -   mean The sample mean calculated from the training data.\n",
      "        .   -   projections The projections of the training data.\n",
      "        .   -   labels The threshold applied in the prediction. If the distance to the nearest neighbor is\n",
      "        .   larger than the threshold, this method returns -1.\n",
      "    \n",
      "    FisherFaceRecognizer_create(...)\n",
      "        FisherFaceRecognizer_create([, num_components[, threshold]]) -> retval\n",
      "        .   @param num_components The number of components (read: Fisherfaces) kept for this Linear\n",
      "        .   Discriminant Analysis with the Fisherfaces criterion. It's useful to keep all components, that\n",
      "        .   means the number of your classes c (read: subjects, persons you want to recognize). If you leave\n",
      "        .   this at the default (0) or set it to a value less-equal 0 or greater (c-1), it will be set to the\n",
      "        .   correct number (c-1) automatically.\n",
      "        .   @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n",
      "        .   is larger than the threshold, this method returns -1.\n",
      "        .   \n",
      "        .   ### Notes:\n",
      "        .   \n",
      "        .   -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n",
      "        .   color spaces.\n",
      "        .   -   **THE FISHERFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n",
      "        .   SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n",
      "        .   input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n",
      "        .   the images.\n",
      "        .   -   This model does not support updating.\n",
      "        .   \n",
      "        .   ### Model internal data:\n",
      "        .   \n",
      "        .   -   num_components see FisherFaceRecognizer::create.\n",
      "        .   -   threshold see FisherFaceRecognizer::create.\n",
      "        .   -   eigenvalues The eigenvalues for this Linear Discriminant Analysis (ordered descending).\n",
      "        .   -   eigenvectors The eigenvectors for this Linear Discriminant Analysis (ordered by their\n",
      "        .   eigenvalue).\n",
      "        .   -   mean The sample mean calculated from the training data.\n",
      "        .   -   projections The projections of the training data.\n",
      "        .   -   labels The labels corresponding to the projections.\n",
      "    \n",
      "    LBPHFaceRecognizer_create(...)\n",
      "        LBPHFaceRecognizer_create([, radius[, neighbors[, grid_x[, grid_y[, threshold]]]]]) -> retval\n",
      "        .   @param radius The radius used for building the Circular Local Binary Pattern. The greater the\n",
      "        .   radius, the\n",
      "        .   @param neighbors The number of sample points to build a Circular Local Binary Pattern from. An\n",
      "        .   appropriate value is to use `8` sample points. Keep in mind: the more sample points you include,\n",
      "        .   the higher the computational cost.\n",
      "        .   @param grid_x The number of cells in the horizontal direction, 8 is a common value used in\n",
      "        .   publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n",
      "        .   feature vector.\n",
      "        .   @param grid_y The number of cells in the vertical direction, 8 is a common value used in\n",
      "        .   publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n",
      "        .   feature vector.\n",
      "        .   @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n",
      "        .   is larger than the threshold, this method returns -1.\n",
      "        .   \n",
      "        .   ### Notes:\n",
      "        .   \n",
      "        .   -   The Circular Local Binary Patterns (used in training and prediction) expect the data given as\n",
      "        .   grayscale images, use cvtColor to convert between the color spaces.\n",
      "        .   -   This model supports updating.\n",
      "        .   \n",
      "        .   ### Model internal data:\n",
      "        .   \n",
      "        .   -   radius see LBPHFaceRecognizer::create.\n",
      "        .   -   neighbors see LBPHFaceRecognizer::create.\n",
      "        .   -   grid_x see LLBPHFaceRecognizer::create.\n",
      "        .   -   grid_y see LBPHFaceRecognizer::create.\n",
      "        .   -   threshold see LBPHFaceRecognizer::create.\n",
      "        .   -   histograms Local Binary Patterns Histograms calculated from the given training data (empty if\n",
      "        .   none was given).\n",
      "        .   -   labels Labels corresponding to the calculated Local Binary Patterns Histograms.\n",
      "    \n",
      "    StandardCollector_create(...)\n",
      "        StandardCollector_create([, threshold]) -> retval\n",
      "        .   @brief Static constructor\n",
      "        .   @param threshold set threshold\n",
      "\n",
      "FILE\n",
      "    (built-in)\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(cv2.face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained sucessefully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Get the training data we previously made\n",
    "data_path = './faces/user/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "# Create arrays for training data and labels\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "# Open training images in our datapath\n",
    "# Create a numpy array for training data\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "\n",
    "# Create a numpy array for both training data and labels\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "# Initialize facial recognizer\n",
    "#model = cv2.face_LBPHFaceRecognizer.create()\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "#model = cv2.\n",
    "# NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "\n",
    "# Let's train our model \n",
    "model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model trained sucessefully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 26.94916681420501)\n",
      "(44, 27.481913267932118)\n",
      "(34, 24.703787413877716)\n",
      "(49, 28.205523774060467)\n",
      "(6, 24.512134333226598)\n",
      "(3, 23.164640788293365)\n",
      "(6, 24.71521917067986)\n",
      "(50, 26.934257963419743)\n",
      "(6, 24.29815538996567)\n",
      "(4, 24.756757250266794)\n",
      "(3, 25.194171870245682)\n",
      "(4, 26.12763672565442)\n",
      "(56, 26.73194107477074)\n",
      "(50, 26.300992927235715)\n",
      "(56, 26.96062699124968)\n",
      "(56, 27.13498339695829)\n",
      "(20, 28.22609917614399)\n",
      "(43, 27.31002133106693)\n",
      "(44, 26.76131598591031)\n",
      "(30, 27.46713281652699)\n",
      "(20, 33.50951208635694)\n",
      "(20, 28.168130582341206)\n",
      "(25, 28.590297354071602)\n",
      "(29, 29.17216222238739)\n",
      "(19, 31.319132244373478)\n",
      "(29, 31.789062584489255)\n",
      "(29, 29.060183660011297)\n",
      "(29, 30.641023491613417)\n",
      "(24, 31.344930976040516)\n",
      "(24, 31.630464975955803)\n",
      "(29, 30.26311374654119)\n",
      "(54, 32.22399614105501)\n",
      "(24, 30.077684732795074)\n",
      "(54, 36.44696530354187)\n",
      "(54, 33.66163096540307)\n",
      "(37, 37.08494437794526)\n",
      "(25, 36.01952089256733)\n",
      "(28, 38.5655865089561)\n",
      "(37, 40.208701697892074)\n",
      "(24, 38.84319473878252)\n",
      "(24, 37.38897148647432)\n",
      "(19, 39.68254234291152)\n",
      "(55, 38.711946088190906)\n",
      "(32, 38.91484888409041)\n",
      "(29, 39.817602341792636)\n",
      "(29, 39.51571606409505)\n",
      "(54, 40.84757819962543)\n",
      "(28, 37.48650057260453)\n",
      "(25, 38.8648025066825)\n",
      "(29, 38.850574705136125)\n",
      "(29, 36.695084812627236)\n",
      "(29, 37.09753177944589)\n",
      "(29, 40.49798644928519)\n",
      "(22, 35.90689523195355)\n",
      "(43, 36.4912071744155)\n",
      "(37, 37.44320360040533)\n",
      "(28, 37.00650884129157)\n",
      "(43, 35.667832883968316)\n",
      "(43, 36.416547277540026)\n",
      "(25, 36.481843671125134)\n",
      "(54, 38.83230186451018)\n",
      "(51, 37.2188744577322)\n",
      "(25, 37.1848552797344)\n",
      "(25, 36.076455423077086)\n",
      "(57, 35.06879105603452)\n",
      "(25, 36.233577357219275)\n",
      "(24, 36.33895999224994)\n",
      "(54, 34.85040546848991)\n",
      "(29, 34.11402668481032)\n",
      "(29, 35.704112817882134)\n",
      "(50, 33.47325814040578)\n",
      "(50, 31.89768186958617)\n",
      "(50, 36.73069145292462)\n",
      "(54, 34.022257703947034)\n",
      "(50, 33.39457871581161)\n",
      "(54, 33.14646441644608)\n",
      "(51, 30.940350831497494)\n",
      "(37, 31.288466831546813)\n",
      "(25, 31.10298634588265)\n",
      "(54, 29.271061920293192)\n",
      "(43, 28.370563255964306)\n",
      "(50, 30.24053193303496)\n",
      "(56, 29.429158867668438)\n",
      "(43, 31.14050436664321)\n",
      "(50, 30.52220698468936)\n",
      "(43, 29.620049870666218)\n",
      "(56, 30.320969960309206)\n",
      "(56, 30.113762506794927)\n",
      "(56, 29.305651282271953)\n",
      "(4, 28.29057891862556)\n",
      "(57, 29.771001969266734)\n",
      "(23, 29.118292886132227)\n",
      "(50, 29.143116668991677)\n",
      "(56, 29.024869710245746)\n",
      "(50, 28.758978055696065)\n",
      "(54, 28.76824311855379)\n",
      "(43, 29.30611238836288)\n",
      "(20, 31.2935230366756)\n",
      "(24, 29.550774206852726)\n",
      "(50, 31.7619156659168)\n",
      "(24, 31.196220073290913)\n",
      "(54, 33.41019838274646)\n",
      "(54, 32.442041285924034)\n",
      "(20, 31.094088649357502)\n",
      "(54, 32.47029252988716)\n",
      "(28, 31.45235527787335)\n",
      "(24, 32.329670584634925)\n",
      "(50, 33.7232688317572)\n",
      "(24, 32.38527578118328)\n",
      "(54, 32.72357473569911)\n",
      "(54, 33.748208839771515)\n",
      "(50, 33.7646908585576)\n",
      "(24, 33.197322847991785)\n",
      "(50, 30.363001707333872)\n",
      "(28, 32.02020552549213)\n",
      "(24, 30.644045502658145)\n",
      "(20, 33.907678586533365)\n",
      "(20, 34.16038776199581)\n",
      "(28, 32.752434861408624)\n",
      "(29, 33.63673411806819)\n",
      "(54, 34.916072700454635)\n",
      "(29, 33.04163487207533)\n",
      "(29, 35.22434955474138)\n",
      "(29, 33.958702291425595)\n",
      "(28, 32.93423731988003)\n",
      "(51, 33.001227450820856)\n",
      "(54, 37.077201788370104)\n",
      "(50, 31.991999176371102)\n",
      "(29, 29.90819011207219)\n",
      "(50, 32.13758112057276)\n",
      "(24, 29.454119116978582)\n",
      "(29, 30.11245586479372)\n",
      "(50, 33.196164782686246)\n",
      "(50, 35.069257005590416)\n",
      "(50, 30.94057196386901)\n",
      "(20, 32.75807435658305)\n",
      "(20, 32.3098897487356)\n",
      "(25, 31.324946071455734)\n",
      "(29, 30.218844720809656)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import webbrowser as wb\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img, []\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi\n",
    "\n",
    "\n",
    "# Open Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    image, face = face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Pass face to prediction model\n",
    "        # \"results\" comprises of a tuple containing the label and the confidence value\n",
    "        results = model.predict(face)\n",
    "        print(results)\n",
    "        if results[1] < 500:\n",
    "            confidence = int( 100 * (1 - (results[1])/400) )\n",
    "            display_string = str(confidence) + '% Confident it is User'\n",
    "            \n",
    "        cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "        \n",
    "        if confidence > 85:\n",
    "            cv2.putText(image, \"Hey Saini\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "            #wb.open('http://localhost:8888/notebooks/faces%20and%20car%20both%20at%20same%20time%20(1).ipynb')\n",
    "            #break\n",
    "        else:\n",
    "            cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "\n",
    "    except:\n",
    "        cv2.putText(image, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
